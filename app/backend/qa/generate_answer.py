import numpy as np
import torch
import os
import redis
from embedder import Embedder
from utils import save_message

from redis.commands.search.query import Query
from endpoint_request import run

embedder = Embedder()

redis_host = os.getenv("REDIS_HOST", "localhost")
redis_port = int(os.getenv("REDIS_PORT", 6379))

redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=False)

MODEL = "llama3-70b-8192"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

class GenerateRAGAnswer:

    def __init__(self):
        """
        Initializes the GenerateRAGAnswer instance with specified course and model settings.

        Parameters:
            course (str): The identifier for the course to retrieve contextual data from.
            model (str): The model identifier for the language model used in answer generation.
        """
        # self.query = ""
        # self.contexts = []  # To store the retrieved contexts
        self.embedder = embedder
        self.query = ""
        self.contexts = []

    def retrieve_contexts(self, query, user_id="user123", chat_id="chat456", top_k=5):
        self.query = query
        INDEX_NAME = f"reference:{user_id}:{chat_id}"
        encoded_query = embedder.embed(self.query)
        query = (
            Query(f'(*)=>[KNN {top_k} @vector $query_vector AS vector_score]')
            .sort_by('vector_score')
            .return_fields('vector_score', 'text')
            .dialect(2)
        )
        self.contexts = redis_client.ft(INDEX_NAME).search(
            query,
            {
            'query_vector': np.array(encoded_query, dtype=np.float32).tobytes()
            }
        ).docs

        # TODO
        # self.contexts = [reference_doc["text"] for reference_doc in reference_docs]

        return

    def gen_prompt(self, query, user_id="user123", chat_id="chat456") -> str:
        """
        Constructs a detailed prompt from the retrieved or searched contexts to be processed by the language model.

        Returns:
            str: A formatted string that serves as a prompt for the language model.
        """
        # Extract the 'text' field from each context dictionary
        self.retrieve_contexts(query, user_id, chat_id)

        context_texts = [ctx["text"] for ctx in self.contexts]

        # Join the extracted text with double newlines
        context = "\n\n".join(context_texts)

        prompt_template = (
            """
            You are a teaching assistant.
            Given a set of relevant information from teacher's recording during the lesson """
            """(delimited by <info></info>), please compose an answer to the question of a student.
            Ensure that the answer is accurate, has a friendly tone, and sounds helpful.
            If you cannot answer, ask the student to clarify the question.
            If no context is available in the system, """
            f"""please answer that you can not find the relevant context in the system.
            <info>
            {context}
            </info>
            Question: {self.query}
            Answer: """
        )

        return prompt_template
    
    async def generate_llm_answer(self, query, user_id="user123", chat_id="chat456"):
        """
        Generates an answer from the language model by streaming content based on the prepared prompt.

        Yields:
            str: Each content chunk generated by the language model.
        """
        final_prompt = self.gen_prompt(query=query, user_id=user_id, chat_id=chat_id)

        # TODO
        final_respond = ""
        for chunk in run(final_prompt, stream=True):
            final_respond += chunk
            yield chunk

        save_message(user_id=user_id, chat_id=chat_id, message=final_respond, role="Assistant")

        



if __name__ == "__main__":
    rag = GenerateRAGAnswer()
    query = "How are you"
    promt = rag.gen_prompt(query=query)
    print(promt)