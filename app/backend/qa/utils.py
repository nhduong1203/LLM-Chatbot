import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from ragas import evaluate
from ragas.metrics import faithfulness
from datasets import Dataset
import os
import redis

from redis.commands.search.field import TextField, VectorField, NumericField
from redis.commands.search.indexDefinition import IndexDefinition, IndexType
from redis.exceptions import ResponseError

VECTOR_DIMENSION = 384


redis_host = os.getenv("REDIS_HOST", "localhost")
redis_port = int(os.getenv("REDIS_PORT", 6379))
redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=False)


def create_redis_index(redis_client, vector_dimension, index_name):
    """
    Creates a Redis index for semantic search if it doesn't already exist.

    Args:
        redis_client (Redis): The Redis client instance.
        vector_dimension (int): The dimension of the vector embeddings.
        index_name (str): The name of the index to create.
    """
    try:
        # Check if the index already exists
        if redis_client.ft(index_name).info():
            print(f"Index '{index_name}' already exists. Skipping creation.")
            return
    except ResponseError:
        # If .info() raises a ResponseError, the index does not exist
        pass

    # Define the schema for the index
    schema = (
        TextField("$.message", no_stem=True, as_name="message"),  # Message content
        NumericField("$.time", as_name="time"),            # Message timestamp
        TextField("$.role", as_name="role"),                # Sender role
        VectorField(
            "$.embedding",  # Embedding field
            "FLAT",         # Flat indexing for simplicity
            {
                "TYPE": "FLOAT32",
                "DIM": vector_dimension,
                "DISTANCE_METRIC": "COSINE",
            },
            as_name="vector",
        ),
    )

    # Define the index prefix to target relevant keys
    definition = IndexDefinition(
        prefix=[index_name],  # Match keys starting with the index prefix
        index_type=IndexType.JSON
    )

    try:
        # Create the index
        redis_client.ft(index_name).create_index(fields=schema, definition=definition)
        print(f"Index '{index_name}' created successfully.")
    except ResponseError as e:
        print(f"Error creating index '{index_name}': {e}")
        raise



def calculate_context_relevancy(query, embedder, contexts):
    """
    Calculates the average cosine similarity between the query's embedding and the embeddings of retrieved contexts.

    Parameters:
        query (str): The query text.
        embedder (object): Embedder object with an `encode` method.
        contexts (list): A list of context dictionaries containing embeddings.

    Returns:
        list: A list of cosine similarity scores for the contexts.
    """
    # Embed the query
    embedded_query = embedder.encode(query)

    # Retrieve the embeddings
    similarity_scores = []

    for context in contexts:
        context_embedding = np.asarray(context["embedding"]).reshape(1, -1)

        # Calculate the cosine similarity
        dot_product = np.dot(context_embedding, embedded_query.T).reshape(-1)
        norm_product = np.linalg.norm(context_embedding) * np.linalg.norm(embedded_query)
        similarity = dot_product / norm_product

        similarity_scores.append(similarity.item())

    return similarity_scores


def calculate_score(query, generated_answer, contexts, model, embedder):
    """
    Computes the faithfulness and relevancy scores for the generated answer based on provided contexts.

    Parameters:
        query (str): The query text.
        generated_answer (str): The answer generated by the language model to evaluate.
        contexts (list): A list of context dictionaries containing text.
        model (object): The language model used for evaluation.
        embedder (object): Embedder object with an `encode` method.

    Returns:
        dict: A dictionary containing 'faithfulness' and 'context_relevancy' scores.
    """
    if not contexts:
        print("Contexts have not been retrieved. Ensure contexts are retrieved before this method is called.")
        return {'faithfulness': 0, 'answer_relevancy': 0}

    # Prepare the dataset for evaluation
    data_samples = {
        'question': [query],
        'answer': [generated_answer],
        'contexts': [[val["text"] for val in contexts]],
    }
    dataset = Dataset.from_dict(data_samples)

    # Evaluate faithfulness
    score = evaluate(dataset, metrics=[faithfulness], llm=model, embeddings=embedder)

    # Calculate context relevancy
    relevancy_score = calculate_context_relevancy(query, embedder, contexts)

    # Convert score to pandas DataFrame and get the mean score
    score_df = score.to_pandas()
    f_score = score_df['faithfulness'].tolist()

    result = {'faithfulness': f_score, 'context_relevancy': relevancy_score}

    return result


def rank_and_select_top_contexts(query, embedder, contexts, top_n=5):
    """
    Ranks and selects the top contexts based on their cosine similarity scores.

    Parameters:
        query (str): The query text.
        embedder (object): Embedder object with an `encode` method.
        contexts (list): A list of context dictionaries containing embeddings.
        top_n (int): Number of top contexts to select.

    Returns:
        list: The top contexts selected based on their overall relevance.
    """
    # Embed the query and reshape it to 2D array
    embedded_query = embedder.encode(query).reshape(1, -1)

    # Extract the embeddings of the contexts and ensure they are in a 2D array
    all_embeddings = [
        np.array(context['embedding']).reshape(1, -1) for context in contexts
    ]
    all_embeddings = np.vstack(all_embeddings)  # Stack to create a 2D array of all embeddings

    # Compute cosine similarity between the query embedding and each context embedding
    similarity_scores = cosine_similarity(embedded_query, all_embeddings)[0]

    # Get the indices of the top N contexts based on similarity scores
    top_indices = similarity_scores.argsort()[-top_n:][::-1]

    # Collect the top contexts based on the computed indices, including their text and metadata
    top_contexts = [contexts[index] for index in top_indices]

    return top_contexts
