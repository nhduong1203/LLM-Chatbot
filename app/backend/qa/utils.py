import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from ragas import evaluate
from ragas.metrics import faithfulness
from datasets import Dataset
import os
import redis

redis_host = os.getenv("REDIS_HOST", "localhost")
redis_port = int(os.getenv("REDIS_PORT", 6379))

redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=False)

def save_message(user_id, chat_id, message, time=None, role="User"):
    pipeline = redis_client.pipeline()
    # Use the current time if no timestamp is provided
    if timestamp is None:
        timestamp = int(time.time())

    # Redis key format
    key = f"history:{user_id}:{chat_id}:{timestamp}"
    data_dict = {
        "message": message,
        "time": timestamp,
        "role": role
    }
    pipeline.json().set(key, "$", data_dict)
    print(f"Message saved under key: {key}")


def calculate_context_relevancy(query, embedder, contexts):
    """
    Calculates the average cosine similarity between the query's embedding and the embeddings of retrieved contexts.

    Parameters:
        query (str): The query text.
        embedder (object): Embedder object with an `encode` method.
        contexts (list): A list of context dictionaries containing embeddings.

    Returns:
        list: A list of cosine similarity scores for the contexts.
    """
    # Embed the query
    embedded_query = embedder.encode(query)

    # Retrieve the embeddings
    similarity_scores = []

    for context in contexts:
        context_embedding = np.asarray(context["embedding"]).reshape(1, -1)

        # Calculate the cosine similarity
        dot_product = np.dot(context_embedding, embedded_query.T).reshape(-1)
        norm_product = np.linalg.norm(context_embedding) * np.linalg.norm(embedded_query)
        similarity = dot_product / norm_product

        similarity_scores.append(similarity.item())

    return similarity_scores


def calculate_score(query, generated_answer, contexts, model, embedder):
    """
    Computes the faithfulness and relevancy scores for the generated answer based on provided contexts.

    Parameters:
        query (str): The query text.
        generated_answer (str): The answer generated by the language model to evaluate.
        contexts (list): A list of context dictionaries containing text.
        model (object): The language model used for evaluation.
        embedder (object): Embedder object with an `encode` method.

    Returns:
        dict: A dictionary containing 'faithfulness' and 'context_relevancy' scores.
    """
    if not contexts:
        print("Contexts have not been retrieved. Ensure contexts are retrieved before this method is called.")
        return {'faithfulness': 0, 'answer_relevancy': 0}

    # Prepare the dataset for evaluation
    data_samples = {
        'question': [query],
        'answer': [generated_answer],
        'contexts': [[val["text"] for val in contexts]],
    }
    dataset = Dataset.from_dict(data_samples)

    # Evaluate faithfulness
    score = evaluate(dataset, metrics=[faithfulness], llm=model, embeddings=embedder)

    # Calculate context relevancy
    relevancy_score = calculate_context_relevancy(query, embedder, contexts)

    # Convert score to pandas DataFrame and get the mean score
    score_df = score.to_pandas()
    f_score = score_df['faithfulness'].tolist()

    result = {'faithfulness': f_score, 'context_relevancy': relevancy_score}

    return result


def rank_and_select_top_contexts(query, embedder, contexts, top_n=5):
    """
    Ranks and selects the top contexts based on their cosine similarity scores.

    Parameters:
        query (str): The query text.
        embedder (object): Embedder object with an `encode` method.
        contexts (list): A list of context dictionaries containing embeddings.
        top_n (int): Number of top contexts to select.

    Returns:
        list: The top contexts selected based on their overall relevance.
    """
    # Embed the query and reshape it to 2D array
    embedded_query = embedder.encode(query).reshape(1, -1)

    # Extract the embeddings of the contexts and ensure they are in a 2D array
    all_embeddings = [
        np.array(context['embedding']).reshape(1, -1) for context in contexts
    ]
    all_embeddings = np.vstack(all_embeddings)  # Stack to create a 2D array of all embeddings

    # Compute cosine similarity between the query embedding and each context embedding
    similarity_scores = cosine_similarity(embedded_query, all_embeddings)[0]

    # Get the indices of the top N contexts based on similarity scores
    top_indices = similarity_scores.argsort()[-top_n:][::-1]

    # Collect the top contexts based on the computed indices, including their text and metadata
    top_contexts = [contexts[index] for index in top_indices]

    return top_contexts
